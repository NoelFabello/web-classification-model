{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87196de-1b5b-48fc-beef-7b78a5449a8b",
   "metadata": {
    "id": "b87196de-1b5b-48fc-beef-7b78a5449a8b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a68952",
   "metadata": {
    "id": "e6a68952"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f515d",
   "metadata": {
    "id": "4e2f515d"
   },
   "source": [
    "Para realizar una comparación entre los modelos clásicos y un modelo actual de procesamiento de lenguaje natural (NLP) se ha escogido el modelo de BERT, ya que es un modelo muy potente con el que es interesante realizar dicha comparación.\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje basado en redes neuronales usado para procesamiento de lenguaje natural . Funciona mediante el pre-entrenamiento en un corpus masivo de texto, lo que le permite capturar el contexto y la semántica del lenguaje de manera profunda. Posteriormente, el modelo puede ser ajustado o \"ajustado fino\" para tareas específicas de NLP, como la clasificación de texto, la extracción de información o la traducción automática. BERT es capaz de comprender el lenguaje en diferentes aplicaciones, lo que lo convierte en un hito en el campo del NLP. Su arquitectura se basa en la red transformer, y su capacidad para codificar el texto y obtener representaciones numéricas precisas lo hace extremadamente versátil y efectivo en una amplia gama de tareas de procesamiento del lenguaje natural.\n",
    "\n",
    "En este trabajo se ha usado BERT para clasificar las páginas se ha usado el modelo BERT-base-uncased y el nlptown/bert-base-multilingual-uncased-sentiment.\n",
    "\n",
    "BERT-base-uncased es una versión de BERT que ha sido pre-entrenada en un corpus masivo de texto en minúsculas. Ofrece la capacidad de comprender el contexto y la semántica del lenguaje en inglés, lo que lo hace adecuado para una amplia gama de tareas de procesamiento del lenguaje natural (NLP). El modelo es \"uncased\", lo que significa que no distingue entre mayúsculas y minúsculas.\n",
    "\n",
    "El modelo \"nlptown/bert-base-multilingual-uncased-sentiment\" es una versión del modelo BERT (Bidirectional Encoder Representations from Transformers) que ha sido pre-entrenada para comprender el sentimiento en texto en varios idiomas. Es una red neuronal profunda que puede analizar el sentimiento en frases y párrafos en diferentes idiomas, lo que lo hace útil para tareas de procesamiento de lenguaje natural en un contexto multilingüe. El modelo es \"uncased\", lo que significa que no distingue entre mayúsculas y minúsculas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b102e2",
   "metadata": {
    "id": "95b102e2"
   },
   "source": [
    "Se descarga una librería de stopwords para la tokenización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dd178-faed-4e61-84f4-fca1376612c0",
   "metadata": {
    "id": "b93dd178-faed-4e61-84f4-fca1376612c0",
    "outputId": "79e81461-4328-49b5-c1d5-4f522ec0da16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jatop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66831afc",
   "metadata": {
    "id": "66831afc"
   },
   "source": [
    "Se cargan los datos con los que va a entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b2056-abb1-475b-af24-89a95add3bbb",
   "metadata": {
    "id": "3f2b2056-abb1-475b-af24-89a95add3bbb"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_json('datosParsed3.json')\n",
    "#display(train_data)\n",
    "# Selecciona una fila, omite la siguiente, y así sucesivamente\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eb012",
   "metadata": {
    "id": "929eb012"
   },
   "source": [
    "# Tokenizado y preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57581fe2",
   "metadata": {
    "id": "57581fe2"
   },
   "source": [
    "Aquí se tokenizan los datos a través de un bertTokenizer para el modelo de ber que vayamos a usar, se filtran las stopwords que se han obtenido anteriormente y se separa entre etiquetas y datos.\n",
    "\n",
    "Además separamos los datos en lo que sería train y test  con una proporción 80% train 20% test. Una vez hecho esto se crean los tensores para poder trabajar con el modelo de bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650986b5-bf60-4a16-9225-eb2f049bbd48",
   "metadata": {
    "id": "650986b5-bf60-4a16-9225-eb2f049bbd48",
    "outputId": "86a8f4cb-85ed-4953-9c96-7e8e07327907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "torch.Size([5298, 512])\n",
      "torch.Size([5298, 512])\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el tokenizador BERT con el modelo de bert que deseamos utilizar\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "train_data['text'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Extraer textos y etiquetas\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "\n",
    "#Separar los datos entre train y test para el entrenamiento del modelo\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizar y convertir a tensores\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# Mostrar información sobre las codificaciones\n",
    "print(train_encodings.keys())\n",
    "print(train_encodings['input_ids'].shape)\n",
    "print(train_encodings['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc66123",
   "metadata": {
    "id": "ccc66123"
   },
   "source": [
    "Para poder manejar los datos de forma cómoda se crea un dataloader, de esta forma tendremos los datos mejor repartidos en lotes y será mas cómodo para nuestro modelo trabajar con los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04426d5-50c6-4646-bbf5-050c43019469",
   "metadata": {
    "id": "c04426d5-50c6-4646-bbf5-050c43019469"
   },
   "outputs": [],
   "source": [
    "# Crear un conjunto de datos personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.label_mapping = {'course': 0, 'department': 1, 'faculty': 2, 'other': 3, 'project': 4, 'staff': 5, 'student': 6}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "        # Convierte la etiqueta a tipo numérico usando el mapeo\n",
    "        item['labels'] = torch.tensor(self.label_mapping[self.labels[idx]])\n",
    "\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9e93c8",
   "metadata": {
    "id": "db9e93c8"
   },
   "source": [
    "Creamos los conjuntos de datos y los dataloaders con la función definida arriba y establecemos el batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10e87b",
   "metadata": {
    "id": "1b10e87b"
   },
   "outputs": [],
   "source": [
    "# Crear conjuntos de datos y DataLoader\n",
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "test_dataset = CustomDataset(test_encodings, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb03c2",
   "metadata": {
    "id": "43eb03c2"
   },
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3f7bc",
   "metadata": {
    "id": "a9c3f7bc"
   },
   "source": [
    "Primero se debe cargar el modelo de bert que deaseamos usar indicándole cuáles van a ser las etiquetas por las que va a tener que clasificar los datos.\n",
    "\n",
    "Para que el modelo funcione de la forma más óptima posible se debería cargar en un dispositivo de GPU, pero en caso de no ser posible se cargara en la CPU.\n",
    "\n",
    "Se define un optimizador para el modelo con los parámetros de la tasa de aprendizaje y la estabilidad numérica deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7745cb-9b86-4762-a75d-eaec4c45a073",
   "metadata": {
    "id": "4c7745cb-9b86-4762-a75d-eaec4c45a073",
    "outputId": "684ba74e-5335-4f9a-db31-785518ec4804"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jatop\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo BERT elegido, bastoacon cambiar el parametro que indica el nombre del modelo\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
    "                                                      num_labels=len(train_data['label'].unique()),\n",
    "                                                      ignore_mismatched_sizes=True) #\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "# Configurar el dispositivo a GPU si está disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Inicializar el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f15e9c",
   "metadata": {
    "id": "99f15e9c"
   },
   "source": [
    "Se inicia el entrenamiento del modelo, para ello se declara el numero de épocas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec4bd5-9e8b-4583-871b-ad0bbd6c5281",
   "metadata": {
    "id": "42ec4bd5-9e8b-4583-871b-ad0bbd6c5281",
    "outputId": "fb2fca2a-e840-4c2a-ca5a-0d4703038559"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatop\\AppData\\Local\\Temp\\ipykernel_13928\\1478713788.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "# Establecer el modelo en modo de entrenamiento\n",
    "model.train()\n",
    "\n",
    "# Definir la función de pérdida (criterio)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Número de épocas (ajusta según sea necesario)\n",
    "num_epochs = 3\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Transferir datos al dispositivo (GPU si está disponible)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Realizar la propagación hacia adelante\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Realizar la retropropagación y la actualización de parámetros\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7ce8d",
   "metadata": {
    "id": "ccb7ce8d"
   },
   "source": [
    "# Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a88f37",
   "metadata": {
    "id": "47a88f37"
   },
   "source": [
    "Una vez que el modelo ha sido entrenado vemos qué resultados hemos obtenido con el conjunto de test, de forma que tendremos una aproximación de cómo de bien o mal ha funcionado nuestro modelo. Para ello se muestra un informe de clasificación con los datos obtenidos después de la evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202646ea-b363-46fc-b55c-fb8862c57619",
   "metadata": {
    "id": "202646ea-b363-46fc-b55c-fb8862c57619",
    "outputId": "f9738a16-bad1-4a9a-e6da-30ad7654faeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jatop\\AppData\\Local\\Temp\\ipykernel_13928\\1478713788.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       577\n",
      "           1       0.98      0.87      0.92       119\n",
      "           2       0.93      0.98      0.96       715\n",
      "           3       0.98      0.96      0.97      2421\n",
      "           4       0.96      0.94      0.95       309\n",
      "           5       0.91      0.82      0.86        87\n",
      "           6       0.95      0.96      0.95      1070\n",
      "\n",
      "    accuracy                           0.96      5298\n",
      "   macro avg       0.95      0.93      0.94      5298\n",
      "weighted avg       0.96      0.96      0.96      5298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cambiar el modelo a modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Listas para almacenar las predicciones y etiquetas reales\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        # Transferir datos al dispositivo (GPU si está disponible)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Realizar la propagación hacia adelante sin realizar la retropropagación\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Obtener las predicciones y las etiquetas reales\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "        # Almacenar las predicciones y las etiquetas reales\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# Calcular y mostrar el informe de clasificación\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122ed9d",
   "metadata": {
    "id": "7122ed9d"
   },
   "source": [
    "# Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bd65b",
   "metadata": {
    "id": "af0bd65b"
   },
   "source": [
    "Preparamos el conjunto de datos que queremos clasificar, de forma similar a como hicimos con los datos de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea1db8-5dad-43b0-8db7-b1858c6927f5",
   "metadata": {
    "id": "3dea1db8-5dad-43b0-8db7-b1858c6927f5"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_json('dataTestParsed3.json')\n",
    "\n",
    "# Preprocesar el conjunto de datos de prueba de la misma manera que el conjunto de entrenamiento\n",
    "test_data['text'] = test_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Extraer textos y etiquetas (si están disponibles en el conjunto de prueba)\n",
    "test_texts = test_data['text']\n",
    "\n",
    "# Tokenizar y convertir a tensores\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e926643",
   "metadata": {
    "id": "1e926643"
   },
   "source": [
    "El modelo, entrenado anteriormente, realiza la clasificación del nuevo conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfa4fa-7430-40bd-a5e8-d454df723c75",
   "metadata": {
    "id": "7fcfa4fa-7430-40bd-a5e8-d454df723c75"
   },
   "outputs": [],
   "source": [
    "# Cambiar el modelo a modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Listas para almacenar las predicciones en el conjunto de prueba sin etiquetas\n",
    "test_preds_without_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_encodings['input_ids'])):\n",
    "        # Transferir datos al dispositivo (GPU si está disponible)\n",
    "        input_ids = test_encodings['input_ids'][i].unsqueeze(0).to(device)\n",
    "        attention_mask = test_encodings['attention_mask'][i].unsqueeze(0).to(device)\n",
    "\n",
    "        # Realizar la propagación hacia adelante sin realizar la retropropagación\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Obtener las predicciones\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # Almacenar las predicciones\n",
    "        test_preds_without_labels.extend(preds)\n",
    "\n",
    "# Obtener los nombres de los archivos (id) del conjunto de prueba\n",
    "test_ids = test_data['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e595a",
   "metadata": {
    "id": "4b2e595a"
   },
   "source": [
    "# Entrega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e5755",
   "metadata": {
    "id": "060e5755"
   },
   "source": [
    "Creamos el documento CSV con el que se compararan la clasificación del modelo con la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dca563-3eec-4be2-9f62-0f3529d8df1c",
   "metadata": {
    "id": "12dca563-3eec-4be2-9f62-0f3529d8df1c",
    "outputId": "0994b7c3-37a3-4673-b805-d671deb6d5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id    label\n",
      "0     aaclkul  student\n",
      "1     aagelci    other\n",
      "2     aangjmn    other\n",
      "3      aawnpc    other\n",
      "4     abdjgiz  student\n",
      "...       ...      ...\n",
      "1654    zxmmn    other\n",
      "1655   zxwkru    other\n",
      "1656  zybimtt    other\n",
      "1657  zypnixf  project\n",
      "1658   zzszho  student\n",
      "\n",
      "[1659 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mapear las predicciones a las etiquetas reales usando el diccionario inverso de la asignación de etiquetas\n",
    "label_mapping = {train_dataset.label_mapping[label]: label for label in train_dataset.label_mapping}\n",
    "\n",
    "# Crear un DataFrame con las predicciones\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': [label_mapping[pred] for pred in test_preds_without_labels]\n",
    "})\n",
    "\n",
    "# Mostrar el DataFrame con las predicciones\n",
    "print(predictions_df)\n",
    "\n",
    "nombre_archivo = 'ENXEBRE-bert-uncased-bs-1.csv'\n",
    "predictions_df.to_csv(nombre_archivo, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f15f04",
   "metadata": {
    "id": "c3f15f04"
   },
   "source": [
    "Guardamos el modelo en un pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd677a92-0db2-47b1-b133-bc4728d1da4f",
   "metadata": {
    "id": "fd677a92-0db2-47b1-b133-bc4728d1da4f"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "# Guardar el modelo\n",
    "model.save_pretrained('C:\\\\Users\\\\jatop\\\\master\\\\Noestructurados')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
