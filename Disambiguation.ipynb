{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from CsvProcessing.CsvProcess import ChosenSynset, CsvProcessor\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class PathCsvProcessor(CsvProcessor):\n",
    "    def processCSV(self, input, output, level = None):\n",
    "        df = pd.read_csv(input)\n",
    "        df['processedMessage'] = df['message'].apply(self._synsetPreprocessRow)\n",
    "        df['processedMessage'] = df['processedMessage'].apply(lambda row: self._PathProcessDisambiguation(row, level))\n",
    "        df.drop(columns='message', axis=1, inplace=True)\n",
    "        df.to_csv(output, index=False)\n",
    "    \n",
    "    #Obtenci칩n de los synsets sustantivos v치lidos de una cadena de texto\n",
    "    def _synsetPreprocessRow(self, sentence):\n",
    "        sentence = re.sub(r'\\b(?:(?:https?|ftp)://|www\\.)\\S+\\b',' ', sentence) \n",
    "        sentence = re.sub('[^a-zA-Z\\s.]+', ' ', sentence)\n",
    "        tokenizedSentence = word_tokenize(sentence)\n",
    "        nounsSentence = self._ObtainNouns(tokenizedSentence)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in nounsSentence if word.lower() not in stop_words]\n",
    "        return filtered_words\n",
    "    \n",
    "    def _PathProcessDisambiguation(self, sentence, level=None):\n",
    "        resultSentence = self._disambiguateRowBySimilarPath(sentence)\n",
    "        toretSentence = []\n",
    "        if level != None:\n",
    "            for i in resultSentence:\n",
    "                if len(i.path) >= level:\n",
    "                    toretSentence.append(i.path[level - 1].name())\n",
    "                else:\n",
    "                    toretSentence.append(i.path[len(i.path) - 1].name())\n",
    "            return toretSentence\n",
    "        return [synset.synset.name() for synset in resultSentence]\n",
    "    \n",
    "    def _disambiguateRowBySimilarPath(self, row):\n",
    "        nouns = self._getNounsFromSentence(row)\n",
    "        resultSentence = []\n",
    "        for i in range(len(nouns)):\n",
    "            previousSynset = ChosenSynset(distance=0)\n",
    "            nextSynset = ChosenSynset(distance=0)\n",
    "            chosenSynset = ChosenSynset(distance=0)\n",
    "            if len(nouns) > 1:\n",
    "                if i >= 1:\n",
    "                    previousSynset = self._getSimilarPath(nouns[i], nouns[i-1])\n",
    "                if i < (len(nouns) - 1):\n",
    "                    nextSynset = self._getSimilarPath(nouns[i], nouns[i+1])\n",
    "            else:\n",
    "                try:\n",
    "                    nextSynset = ChosenSynset(nouns[i][0], path=max(nouns[i][0].hypernym_paths(), key=len), distance=1)\n",
    "                except Exception:\n",
    "                    nextSynset = ChosenSynset(nouns[i][0], path=[])\n",
    "            for synset in [previousSynset,nextSynset]:\n",
    "                if synset.distance > chosenSynset.distance:\n",
    "                    chosenSynset = synset\n",
    "            if chosenSynset.synset is not None:\n",
    "                resultSentence.append(chosenSynset)\n",
    "        return resultSentence\n",
    "    \n",
    "    # Obtiene el synset de synArray com m치s similitud a una acepci칩n de comparedSynArray\n",
    "    def _getSimilarPath(self, synArray, comparedSynArray):\n",
    "        result = ChosenSynset(distance=0)\n",
    "        for i in synArray:\n",
    "            for j in comparedSynArray:\n",
    "                distance = i.path_similarity(j)\n",
    "                if distance > result.distance:\n",
    "                    result.synset = i\n",
    "                    result.distance = distance\n",
    "                    result.comparedWord = j\n",
    "                    result.path = max(i.hypernym_paths(), key=len)\n",
    "        return result\n",
    "    \n",
    "    def _ObtainNouns(self, sentence):\n",
    "        taggedSentence = pos_tag(sentence)\n",
    "        nounsSentence = [word for word, tag in taggedSentence if tag == \"NN\" or tag == \"NNS\"]\n",
    "        resultSentence = [word for word in nounsSentence if len(wn.synsets(word)) > 0]\n",
    "        return resultSentence\n",
    "    \n",
    "    def _getNounsFromSentence(self, array):\n",
    "        validSynset = []\n",
    "        for i in array:\n",
    "            tempNouns = wn.synsets(i)\n",
    "            tempNouns = self._getNounsFromSynset(tempNouns)\n",
    "            if len(tempNouns) > 0:\n",
    "                validSynset.append(tempNouns)\n",
    "        return validSynset\n",
    "\n",
    "    def _getNounsFromSynset(self, array):\n",
    "            verbPattern = r\"\\.n\\.\"\n",
    "            validSynset = []\n",
    "            for i in array:\n",
    "                name = i.name()\n",
    "                if(re.search(verbPattern,name)):\n",
    "                        validSynset.append(i)\n",
    "            return validSynset"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
