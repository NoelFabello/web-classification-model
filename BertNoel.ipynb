{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Verifica si hay una GPU disponible y, en caso contrario, utiliza la CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Carga el modelo pre-entrenado y el tokenizador\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5).to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('dataTrain.json')\n",
    "df_test = pd.read_json('dataTest.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets\n",
      "dataloaders\n",
      "opitmizer\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que `texts` es una lista de textos y `labels` es una lista de etiquetas\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(df.text, df.label, test_size=0.2)\n",
    "\n",
    "# Codifica los textos\n",
    "train_encodings = tokenizer(texts_train.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(texts_val.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convertir las representaciones tokenizadas a tensores\n",
    "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
    "\n",
    "val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
    "val_attention_mask = torch.tensor(val_encodings['attention_mask'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(labels_train)\n",
    "val_labels_encoded = label_encoder.transform(labels_val)\n",
    "\n",
    "# Convierte las etiquetas a tensores\n",
    "train_labels = torch.tensor(train_labels_encoded)\n",
    "val_labels = torch.tensor(val_labels_encoded)\n",
    "\n",
    "# Crea los conjuntos de datos\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "print('datasets')\n",
    "\n",
    "\n",
    "# Crea los dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=1)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=1)\n",
    "\n",
    "print('dataloaders')\n",
    "\n",
    "# Define el optimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "print('opitmizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Entrena el modelo\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carga el modelo BERT preentrenado\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Inicializa la lista para las predicciones\n",
    "predictions = []\n",
    "\n",
    "# Cambia al modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Itera sobre los datos de prueba\n",
    "for item in df_test:\n",
    "    # Tokeniza el texto\n",
    "    inputs = tokenizer(item['text'], return_tensors='pt')\n",
    "    \n",
    "    # Realiza una pasada hacia adelante (predice)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obtiene las predicciones\n",
    "    logits = outputs[0]\n",
    "    predicted_class = torch.argmax(logits, dim=1)\n",
    "    \n",
    "    # Almacena las predicciones\n",
    "    predictions.append(predicted_class.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo en el conjunto de validación es: 1.000\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           1.00        40\n",
      "   macro avg       1.00      1.00      1.00        40\n",
      "weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cambia al modo de evaluación\n",
    "model.eval()\n",
    "\n",
    "# Inicializa las listas para las verdaderas y las predicciones\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Itera sobre los lotes en el dataloader de validación\n",
    "for batch in validation_dataloader:\n",
    "    # Agrega el lote al dispositivo\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Desempaqueta los inputs del dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # No calcula ni almacena los gradientes para ahorrar memoria y mejorar la velocidad\n",
    "    with torch.no_grad():\n",
    "        # Realiza una pasada hacia adelante (predice)\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Obtiene las predicciones\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Mueve los logits y las etiquetas a la CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    # Almacena las verdaderas y las predicciones\n",
    "    true_labels.append(label_ids)\n",
    "    pred_labels.append(np.argmax(logits, axis=1))\n",
    "\n",
    "# Aplana las listas de verdaderas y predicciones\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "\n",
    "# Calcula la precisión\n",
    "acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "print('La precisión del modelo en el conjunto de validación es: {:.3f}'.format(acc))\n",
    "print('Reporte de clasificación:')\n",
    "print(classification_report(true_labels, pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
